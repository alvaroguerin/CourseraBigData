Apache	1
Hadoop	1
is	1
an	1
open-source	1
software	1
framework	1
written	1
in	1
Java	1
for	1
distributed	1
storage	1
and	1
distributed	1
processing	1
of	1
very	1
large	1
data	1
sets	1
on	1
computer	1
clusters	1
built	1
from	1
commodity	1
hardware.	1
All	1
the	1
modules	1
in	1
Hadoop	1
are	1
designed	1
with	1
a	1
fundamental	1
assumption	1
that	1
hardware	1
failures	1
are	1
common	1
and	1
should	1
be	1
automatically	1
handled	1
by	1
the	1
framework.[3]	1
The	1
core	1
of	1
Apache	1
Hadoop	1
consists	1
of	1
a	1
storage	1
part,	1
known	1
as	1
Hadoop	1
Distributed	1
File	1
System	1
(HDFS),	1
and	1
a	1
processing	1
part	1
called	1
MapReduce.	1
Hadoop	1
splits	1
files	1
into	1
large	1
blocks	1
and	1
distributes	1
them	1
across	1
nodes	1
in	1
a	1
cluster.	1
To	1
process	1
data,	1
Hadoop	1
transfers	1
packaged	1
code	1
for	1
nodes	1
to	1
process	1
in	1
parallel	1
based	1
on	1
the	1
data	1
that	1
needs	1
to	1
be	1
processed.	1
This	1
approach	1
takes	1
advantage	1
of	1
data	1
locality[4]—	1
nodes	1
manipulating	1
the	1
data	1
they	1
have	1
access	1
to—	1
to	1
allow	1
the	1
dataset	1
to	1
be	1
processed	1
faster	1
and	1
more	1
efficiently	1
than	1
it	1
would	1
be	1
in	1
a	1
more	1
conventional	1
supercomputer	1
architecture	1
that	1
relies	1
on	1
a	1
parallel	1
file	1
system	1
where	1
computation	1
and	1
data	1
are	1
distributed	1
via	1
high-speed	1
networking.[5]	1
The	1
base	1
Apache	1
Hadoop	1
framework	1
is	1
composed	1
of	1
the	1
following	1
modules:	1
Hadoop	1
Common	1
–	1
contains	1
libraries	1
and	1
utilities	1
needed	1
by	1
other	1
Hadoop	1
modules;	1
Hadoop	1
Distributed	1
File	1
System	1
(HDFS)	1
–	1
a	1
distributed	1
file-system	1
that	1
stores	1
data	1
on	1
commodity	1
machines,	1
providing	1
very	1
high	1
aggregate	1
bandwidth	1
across	1
the	1
cluster;	1
Hadoop	1
YARN	1
–	1
a	1
resource-management	1
platform	1
responsible	1
for	1
managing	1
computing	1
resources	1
in	1
clusters	1
and	1
using	1
them	1
for	1
scheduling	1
of	1
users'	1
applications;[6][7]	1
and	1
Hadoop	1
MapReduce	1
–	1
an	1
implementation	1
of	1
the	1
MapReduce	1
programming	1
model	1
for	1
large	1
scale	1
data	1
processing.	1
The	1
term	1
Hadoop	1
has	1
come	1
to	1
refer	1
not	1
just	1
to	1
the	1
base	1
modules	1
above,	1
but	1
also	1
to	1
the	1
ecosystem,[8]	1
or	1
collection	1
of	1
additional	1
software	1
packages	1
that	1
can	1
be	1
installed	1
on	1
top	1
of	1
or	1
alongside	1
Hadoop,	1
such	1
as	1
Apache	1
Pig,	1
Apache	1
Hive,	1
Apache	1
HBase,	1
Apache	1
Phoenix,	1
Apache	1
Spark,	1
Apache	1
ZooKeeper,	1
Cloudera	1
Impala,	1
Apache	1
Flume,	1
Apache	1
Sqoop,	1
Apache	1
Oozie,	1
Apache	1
Storm.[9]	1
Apache	1
Hadoop's	1
MapReduce	1
and	1
HDFS	1
components	1
were	1
inspired	1
by	1
Google	1
papers	1
on	1
their	1
MapReduce	1
and	1
Google	1
File	1
System.[10]	1
History	1
Hadoop	1
was	1
created	1
by	1
Doug	1
Cutting	1
and	1
Mike	1
Cafarella[12]	1
in	1
2005.	1
Cutting,	1
who	1
was	1
working	1
at	1
Yahoo!	1
at	1
the	1
time,[13]	1
named	1
it	1
after	1
his	1
son's	1
toy	1
elephant.[14]	1
It	1
was	1
originally	1
developed	1
to	1
support	1
distribution	1
for	1
the	1
Nutch	1
search	1
engine	1
project.[15]	1
Architecture	1
See	1
also:	1
Hadoop	1
Distributed	1
File	1
System,	1
Apache	1
HBase	1
and	1
MapReduce	1
Hadoop	1
consists	1
of	1
the	1
Hadoop	1
Common	1
package,	1
which	1
provides	1
filesystem	1
and	1
OS	1
level	1
abstractions,	1
a	1
MapReduce	1
engine	1
(either	1
MapReduce/MR1	1
or	1
YARN/MR2)[16]	1
and	1
the	1
Hadoop	1
Distributed	1
File	1
System	1
(HDFS).	1
The	1
Hadoop	1
Common	1
package	1
contains	1
the	1
necessary	1
Java	1
ARchive	1
(JAR)	1
files	1
and	1
scripts	1
needed	1
to	1
start	1
Hadoop.	1
For	1
effective	1
scheduling	1
of	1
work,	1
every	1
Hadoop-compatible	1
file	1
system	1
should	1
provide	1
location	1
awareness:	1
the	1
name	1
of	1
the	1
rack	1
(more	1
precisely,	1
of	1
the	1
network	1
switch)	1
where	1
a	1
worker	1
node	1
is.	1
Hadoop	1
applications	1
can	1
use	1
this	1
information	1
to	1
execute	1
code	1
on	1
the	1
node	1
where	1
the	1
data	1
is,	1
and,	1
failing	1
that,	1
on	1
the	1
same	1
rack/switch	1
to	1
reduce	1
backbone	1
traffic.	1
HDFS	1
uses	1
this	1
method	1
when	1
replicating	1
data	1
for	1
data	1
redundancy	1
across	1
multiple	1
racks.	1
This	1
approach	1
reduces	1
the	1
impact	1
of	1
a	1
rack	1
power	1
outage	1
or	1
switch	1
failure;	1
if	1
one	1
of	1
these	1
hardware	1
failures	1
occurs,	1
the	1
data	1
will	1
remain	1
available.[17]	1
Hadoop	1
cluster	1
A	1
multi-node	1
Hadoop	1
cluster	1
A	1
small	1
Hadoop	1
cluster	1
includes	1
a	1
single	1
master	1
and	1
multiple	1
worker	1
nodes.	1
The	1
master	1
node	1
consists	1
of	1
a	1
Job	1
Tracker,	1
Task	1
Tracker,	1
NameNode,	1
and	1
DataNode.	1
A	1
slave	1
or	1
worker	1
node	1
acts	1
as	1
both	1
a	1
DataNode	1
and	1
TaskTracker,	1
though	1
it	1
is	1
possible	1
to	1
have	1
data-only	1
worker	1
nodes	1
and	1
compute-only	1
worker	1
nodes.	1
These	1
are	1
normally	1
used	1
only	1
in	1
nonstandard	1
applications.[18]	1
Hadoop	1
requires	1
Java	1
Runtime	1
Environment	1
(JRE)	1
1.6	1
or	1
higher.	1
The	1
standard	1
startup	1
and	1
shutdown	1
scripts	1
require	1
that	1
Secure	1
Shell	1
(ssh)	1
be	1
set	1
up	1
between	1
nodes	1
in	1
the	1
cluster.[19]	1
In	1
a	1
larger	1
cluster,	1
HDFS	1
nodes	1
are	1
managed	1
through	1
a	1
dedicated	1
NameNode	1
server	1
to	1
host	1
the	1
file	1
system	1
index,	1
and	1
a	1
secondary	1
NameNode	1
that	1
can	1
generate	1
snapshots	1
of	1
the	1
namenode's	1
memory	1
structures,	1
thereby	1
preventing	1
file-system	1
corruption	1
and	1
loss	1
of	1
data.	1
Similarly,	1
a	1
standalone	1
JobTracker	1
server	1
can	1
manage	1
job	1
scheduling	1
across	1
nodes.	1
When	1
Hadoop	1
MapReduce	1
is	1
used	1
with	1
an	1
alternate	1
file	1
system,	1
the	1
NameNode,	1
secondary	1
NameNode,	1
and	1
DataNode	1
architecture	1
of	1
HDFS	1
are	1
replaced	1
by	1
the	1
file-system-specific	1
equivalents.	1
File	1
systems	1
Hadoop	1
distributed	1
file	1
system	1
The	1
Hadoop	1
distributed	1
file	1
system	1
(HDFS)	1
is	1
a	1
distributed,	1
scalable,	1
and	1
portable	1
file-system	1
written	1
in	1
Java	1
for	1
the	1
Hadoop	1
framework.	1
A	1
Hadoop	1
cluster	1
has	1
nominally	1
a	1
single	1
namenode	1
plus	1
a	1
cluster	1
of	1
datanodes,	1
although	1
redundancy	1
options	1
are	1
available	1
for	1
the	1
namenode	1
due	1
to	1
its	1
criticality.	1
Each	1
datanode	1
serves	1
up	1
blocks	1
of	1
data	1
over	1
the	1
network	1
using	1
a	1
block	1
protocol	1
specific	1
to	1
HDFS.	1
The	1
file	1
system	1
uses	1
TCP/IP	1
sockets	1
for	1
communication.	1
Clients	1
use	1
remote	1
procedure	1
call	1
(RPC)	1
to	1
communicate	1
between	1
each	1
other.	1
HDFS	1
stores	1
large	1
files	1
(typically	1
in	1
the	1
range	1
of	1
gigabytes	1
to	1
terabytes[20])	1
across	1
multiple	1
machines.	1
It	1
achieves	1
reliability	1
by	1
replicating	1
the	1
data	1
across	1
multiple	1
hosts,	1
and	1
hence	1
theoretically	1
does	1
not	1
require	1
RAID	1
storage	1
on	1
hosts	1
(but	1
to	1
increase	1
I/O	1
performance	1
some	1
RAID	1
configurations	1
are	1
still	1
useful).	1
With	1
the	1
default	1
replication	1
value,	1
3,	1
data	1
is	1
stored	1
on	1
three	1
nodes:	1
two	1
on	1
the	1
same	1
rack,	1
and	1
one	1
on	1
a	1
different	1
rack.	1
Data	1
nodes	1
can	1
talk	1
to	1
each	1
other	1
to	1
rebalance	1
data,	1
to	1
move	1
copies	1
around,	1
and	1
to	1
keep	1
the	1
replication	1
of	1
data	1
high.	1
HDFS	1
is	1
not	1
fully	1
POSIX-compliant,	1
because	1
the	1
requirements	1
for	1
a	1
POSIX	1
file-system	1
differ	1
from	1
the	1
target	1
goals	1
for	1
a	1
Hadoop	1
application.	1
The	1
trade-off	1
of	1
not	1
having	1
a	1
fully	1
POSIX-compliant	1
file-system	1
is	1
increased	1
performance	1
for	1
data	1
throughput	1
and	1
support	1
for	1
non-POSIX	1
operations	1
such	1
as	1
Append.[21]	1
HDFS	1
added	1
the	1
high-availability	1
capabilities,	1
as	1
announced	1
for	1
release	1
2.0	1
in	1
May	1
2012,[22]	1
letting	1
the	1
main	1
metadata	1
server	1
(the	1
NameNode)	1
fail	1
over	1
manually	1
to	1
a	1
backup.	1
The	1
project	1
has	1
also	1
started	1
developing	1
automatic	1
fail-over.	1
The	1
HDFS	1
file	1
system	1
includes	1
a	1
so-called	1
secondary	1
namenode,	1
a	1
misleading	1
name	1
that	1
some	1
might	1
incorrectly	1
interpret	1
as	1
a	1
backup	1
namenode	1
for	1
when	1
the	1
primary	1
namenode	1
goes	1
offline.	1
In	1
fact,	1
the	1
secondary	1
namenode	1
regularly	1
connects	1
with	1
the	1
primary	1
namenode	1
and	1
builds	1
snapshots	1
of	1
the	1
primary	1
namenode's	1
directory	1
information,	1
which	1
the	1
system	1
then	1
saves	1
to	1
local	1
or	1
remote	1
directories.	1
These	1
checkpointed	1
images	1
can	1
be	1
used	1
to	1
restart	1
a	1
failed	1
primary	1
namenode	1
without	1
having	1
to	1
replay	1
the	1
entire	1
journal	1
of	1
file-system	1
actions,	1
then	1
to	1
edit	1
the	1
log	1
to	1
create	1
an	1
up-to-date	1
directory	1
structure.	1
Because	1
the	1
namenode	1
is	1
the	1
single	1
point	1
for	1
storage	1
and	1
management	1
of	1
metadata,	1
it	1
can	1
become	1
a	1
bottleneck	1
for	1
supporting	1
a	1
huge	1
number	1
of	1
files,	1
especially	1
a	1
large	1
number	1
of	1
small	1
files.	1
HDFS	1
Federation,	1
a	1
new	1
addition,	1
aims	1
to	1
tackle	1
this	1
problem	1
to	1
a	1
certain	1
extent	1
by	1
allowing	1
multiple	1
namespaces	1
served	1
by	1
separate	1
namenodes.	1
An	1
advantage	1
of	1
using	1
HDFS	1
is	1
data	1
awareness	1
between	1
the	1
job	1
tracker	1
and	1
task	1
tracker.	1
The	1
job	1
tracker	1
schedules	1
map	1
or	1
reduce	1
jobs	1
to	1
task	1
trackers	1
with	1
an	1
awareness	1
of	1
the	1
data	1
location.	1
For	1
example:	1
if	1
node	1
A	1
contains	1
data	1
(x,y,z)	1
and	1
node	1
B	1
contains	1
data	1
(a,b,c),	1
the	1
job	1
tracker	1
schedules	1
node	1
B	1
to	1
perform	1
map	1
or	1
reduce	1
tasks	1
on	1
(a,b,c)	1
and	1
node	1
A	1
would	1
be	1
scheduled	1
to	1
perform	1
map	1
or	1
reduce	1
tasks	1
on	1
(x,y,z).	1
This	1
reduces	1
the	1
amount	1
of	1
traffic	1
that	1
goes	1
over	1
the	1
network	1
and	1
prevents	1
unnecessary	1
data	1
transfer.	1
When	1
Hadoop	1
is	1
used	1
with	1
other	1
file	1
systems,	1
this	1
advantage	1
is	1
not	1
always	1
available.	1
This	1
can	1
have	1
a	1
significant	1
impact	1
on	1
job-completion	1
times,	1
which	1
has	1
been	1
demonstrated	1
when	1
running	1
data-intensive	1
jobs.[23]	1
HDFS	1
was	1
designed	1
for	1
mostly	1
immutable	1
files[21]	1
and	1
may	1
not	1
be	1
suitable	1
for	1
systems	1
requiring	1
concurrent	1
write-operations.	1
HDFS	1
can	1
be	1
mounted	1
directly	1
with	1
a	1
Filesystem	1
in	1
Userspace	1
(FUSE)	1
virtual	1
file	1
system	1
on	1
Linux	1
and	1
some	1
other	1
Unix	1
systems.	1
File	1
access	1
can	1
be	1
achieved	1
through	1
the	1
native	1
Java	1
application	1
programming	1
interface	1
(API),	1
the	1
Thrift	1
API	1
to	1
generate	1
a	1
client	1
in	1
the	1
language	1
of	1
the	1
users'	1
choosing	1
(C++,	1
Java,	1
Python,	1
PHP,	1
Ruby,	1
Erlang,	1
Perl,	1
Haskell,	1
C#,	1
Cocoa,	1
Smalltalk,	1
and	1
OCaml),	1
the	1
command-line	1
interface,	1
browsed	1
through	1
the	1
HDFS-UI	1
Web	1
application	1
(webapp)	1
over	1
HTTP,	1
or	1
via	1
3rd-party	1
network	1
client	1
libraries.[24]	1
Other	1
file	1
systems	1
Hadoop	1
works	1
directly	1
with	1
any	1
distributed	1
file	1
system	1
that	1
can	1
be	1
mounted	1
by	1
the	1
underlying	1
operating	1
system	1
simply	1
by	1
using	1
a	1
file://	1
URL;	1
however,	1
this	1
comes	1
at	1
a	1
price:	1
the	1
loss	1
of	1
locality.	1
To	1
reduce	1
network	1
traffic,	1
Hadoop	1
needs	1
to	1
know	1
which	1
servers	1
are	1
closest	1
to	1
the	1
data;	1
this	1
is	1
information	1
that	1
Hadoop-specific	1
file	1
system	1
bridges	1
can	1
provide.	1
In	1
May	1
2011,	1
the	1
list	1
of	1
supported	1
file	1
systems	1
bundled	1
with	1
Apache	1
Hadoop	1
were:	1
HDFS:	1
Hadoop's	1
own	1
rack-aware	1
file	1
system.[25]	1
This	1
is	1
designed	1
to	1
scale	1
to	1
tens	1
of	1
petabytes	1
of	1
storage	1
and	1
runs	1
on	1
top	1
of	1
the	1
file	1
systems	1
of	1
the	1
underlying	1
operating	1
systems.	1
FTP	1
File	1
system:	1
this	1
stores	1
all	1
its	1
data	1
on	1
remotely	1
accessible	1
FTP	1
servers.	1
Amazon	1
S3	1
(Simple	1
Storage	1
Service)	1
file	1
system.	1
This	1
is	1
targeted	1
at	1
clusters	1
hosted	1
on	1
the	1
Amazon	1
Elastic	1
Compute	1
Cloud	1
server-on-demand	1
infrastructure.	1
There	1
is	1
no	1
rack-awareness	1
in	1
this	1
file	1
system,	1
as	1
it	1
is	1
all	1
remote.	1
Windows	1
Azure	1
Storage	1
Blobs	1
(WASB)	1
file	1
system.	1
WASB,	1
an	1
extension	1
on	1
top	1
of	1
HDFS,	1
allows	1
distributions	1
of	1
Hadoop	1
to	1
access	1
data	1
in	1
Azure	1
blob	1
stores	1
without	1
moving	1
the	1
data	1
permanently	1
into	1
the	1
cluster.	1
A	1
number	1
of	1
third-party	1
file	1
system	1
bridges	1
have	1
also	1
been	1
written,	1
none	1
of	1
which	1
are	1
currently	1
in	1
Hadoop	1
distributions.	1
However,	1
some	1
commercial	1
distributions	1
of	1
Hadoop	1
ship	1
with	1
an	1
alternative	1
filesystem	1
as	1
the	1
default—specifically	1
IBM	1
and	1
MapR.	1
In	1
2009,	1
IBM	1
discussed	1
running	1
Hadoop	1
over	1
the	1
IBM	1
General	1
Parallel	1
File	1
System.[26]	1
The	1
source	1
code	1
was	1
published	1
in	1
October	1
2009.[27]	1
In	1
April	1
2010,	1
Parascale	1
published	1
the	1
source	1
code	1
to	1
run	1
Hadoop	1
against	1
the	1
Parascale	1
file	1
system.[28]	1
In	1
April	1
2010,	1
Appistry	1
released	1
a	1
Hadoop	1
file	1
system	1
driver	1
for	1
use	1
with	1
its	1
own	1
CloudIQ	1
Storage	1
product.[29]	1
In	1
June	1
2010,	1
HP	1
discussed	1
a	1
location-aware	1
IBRIX	1
Fusion	1
file	1
system	1
driver.[30]	1
In	1
May	1
2011,	1
MapR	1
Technologies,	1
Inc.	1
announced	1
the	1
availability	1
of	1
an	1
alternative	1
file	1
system	1
for	1
Hadoop,	1
which	1
replaced	1
the	1
HDFS	1
file	1
system	1
with	1
a	1
full	1
random-access	1
read/write	1
file	1
system.	1
JobTracker	1
and	1
TaskTracker:	1
the	1
MapReduce	1
engine	1
Main	1
article:	1
MapReduce	1
Above	1
the	1
file	1
systems	1
comes	1
the	1
MapReduce	1
Engine,	1
which	1
consists	1
of	1
one	1
JobTracker,	1
to	1
which	1
client	1
applications	1
submit	1
MapReduce	1
jobs.	1
The	1
JobTracker	1
pushes	1
work	1
out	1
to	1
available	1
TaskTracker	1
nodes	1
in	1
the	1
cluster,	1
striving	1
to	1
keep	1
the	1
work	1
as	1
close	1
to	1
the	1
data	1
as	1
possible.	1
With	1
a	1
rack-aware	1
file	1
system,	1
the	1
JobTracker	1
knows	1
which	1
node	1
contains	1
the	1
data,	1
and	1
which	1
other	1
machines	1
are	1
nearby.	1
If	1
the	1
work	1
cannot	1
be	1
hosted	1
on	1
the	1
actual	1
node	1
where	1
the	1
data	1
resides,	1
priority	1
is	1
given	1
to	1
nodes	1
in	1
the	1
same	1
rack.	1
This	1
reduces	1
network	1
traffic	1
on	1
the	1
main	1
backbone	1
network.	1
If	1
a	1
TaskTracker	1
fails	1
or	1
times	1
out,	1
that	1
part	1
of	1
the	1
job	1
is	1
rescheduled.	1
The	1
TaskTracker	1
on	1
each	1
node	1
spawns	1
a	1
separate	1
Java	1
Virtual	1
Machine	1
process	1
to	1
prevent	1
the	1
TaskTracker	1
itself	1
from	1
failing	1
if	1
the	1
running	1
job	1
crashes	1
its	1
JVM.	1
A	1
heartbeat	1
is	1
sent	1
from	1
the	1
TaskTracker	1
to	1
the	1
JobTracker	1
every	1
few	1
minutes	1
to	1
check	1
its	1
status.	1
The	1
Job	1
Tracker	1
and	1
TaskTracker	1
status	1
and	1
information	1
is	1
exposed	1
by	1
Jetty	1
and	1
can	1
be	1
viewed	1
from	1
a	1
web	1
browser.	1
Known	1
limitations	1
of	1
this	1
approach	1
are:	1
The	1
allocation	1
of	1
work	1
to	1
TaskTrackers	1
is	1
very	1
simple.	1
Every	1
TaskTracker	1
has	1
a	1
number	1
of	1
available	1
slots	1
(such	1
as	1
"4	1
slots").	1
Every	1
active	1
map	1
or	1
reduce	1
task	1
takes	1
up	1
one	1
slot.	1
The	1
Job	1
Tracker	1
allocates	1
work	1
to	1
the	1
tracker	1
nearest	1
to	1
the	1
data	1
with	1
an	1
available	1
slot.	1
There	1
is	1
no	1
consideration	1
of	1
the	1
current	1
system	1
load	1
of	1
the	1
allocated	1
machine,	1
and	1
hence	1
its	1
actual	1
availability.	1
If	1
one	1
TaskTracker	1
is	1
very	1
slow,	1
it	1
can	1
delay	1
the	1
entire	1
MapReduce	1
job—especially	1
towards	1
the	1
end	1
of	1
a	1
job,	1
where	1
everything	1
can	1
end	1
up	1
waiting	1
for	1
the	1
slowest	1
task.	1
With	1
speculative	1
execution	1
enabled,	1
however,	1
a	1
single	1
task	1
can	1
be	1
executed	1
on	1
multiple	1
slave	1
nodes.	1
Scheduling	1
By	1
default	1
Hadoop	1
uses	1
FIFO	1
scheduling,	1
and	1
optionally	1
5	1
scheduling	1
priorities	1
to	1
schedule	1
jobs	1
from	1
a	1
work	1
queue.[31]	1
In	1
version	1
0.19	1
the	1
job	1
scheduler	1
was	1
refactored	1
out	1
of	1
the	1
JobTracker,	1
while	1
adding	1
the	1
ability	1
to	1
use	1
an	1
alternate	1
scheduler	1
(such	1
as	1
the	1
Fair	1
scheduler	1
or	1
the	1
Capacity	1
scheduler,	1
described	1
next).[32]	1
Fair	1
scheduler	1
The	1
fair	1
scheduler	1
was	1
developed	1
by	1
Facebook.[33]	1
The	1
goal	1
of	1
the	1
fair	1
scheduler	1
is	1
to	1
provide	1
fast	1
response	1
times	1
for	1
small	1
jobs	1
and	1
QoS	1
for	1
production	1
jobs.	1
The	1
fair	1
scheduler	1
has	1
three	1
basic	1
concepts.[34]	1
Jobs	1
are	1
grouped	1
into	1
pools.	1
Each	1
pool	1
is	1
assigned	1
a	1
guaranteed	1
minimum	1
share.	1
Excess	1
capacity	1
is	1
split	1
between	1
jobs.	1
By	1
default,	1
jobs	1
that	1
are	1
uncategorized	1
go	1
into	1
a	1
default	1
pool.	1
Pools	1
have	1
to	1
specify	1
the	1
minimum	1
number	1
of	1
map	1
slots,	1
reduce	1
slots,	1
and	1
a	1
limit	1
on	1
the	1
number	1
of	1
running	1
jobs.	1
Capacity	1
scheduler	1
The	1
capacity	1
scheduler	1
was	1
developed	1
by	1
Yahoo.	1
The	1
capacity	1
scheduler	1
supports	1
several	1
features	1
that	1
are	1
similar	1
to	1
the	1
fair	1
scheduler.[35]	1
Queues	1
are	1
allocated	1
a	1
fraction	1
of	1
the	1
total	1
resource	1
capacity.	1
Free	1
resources	1
are	1
allocated	1
to	1
queues	1
beyond	1
their	1
total	1
capacity.	1
Within	1
a	1
queue	1
a	1
job	1
with	1
a	1
high	1
level	1
of	1
priority	1
has	1
access	1
to	1
the	1
queue's	1
resources.	1
There	1
is	1
no	1
preemption	1
once	1
a	1
job	1
is	1
running.	1
Other	1
applications	1
The	1
HDFS	1
file	1
system	1
is	1
not	1
restricted	1
to	1
MapReduce	1
jobs.	1
It	1
can	1
be	1
used	1
for	1
other	1
applications,	1
many	1
of	1
which	1
are	1
under	1
development	1
at	1
Apache.	1
The	1
list	1
includes	1
the	1
HBase	1
database,	1
the	1
Apache	1
Mahout	1
machine	1
learning	1
system,	1
and	1
the	1
Apache	1
Hive	1
Data	1
Warehouse	1
system.	1
Hadoop	1
can	1
in	1
theory	1
be	1
used	1
for	1
any	1
sort	1
of	1
work	1
that	1
is	1
batch-oriented	1
rather	1
than	1
real-time,	1
is	1
very	1
data-intensive,	1
and	1
benefits	1
from	1
parallel	1
processing	1
of	1
data.	1
It	1
can	1
also	1
be	1
used	1
to	1
complement	1
a	1
real-time	1
system,	1
such	1
as	1
lambda	1
architecture.	1
As	1
of	1
October	1
2009,	1
commercial	1
applications	1
of	1
Hadoop[36]	1
included:	1
Log	1
and/or	1
clickstream	1
analysis	1
of	1
various	1
kinds	1
Marketing	1
analytics	1
Machine	1
learning	1
and/or	1
sophisticated	1
data	1
mining	1
Image	1
processing	1
Processing	1
of	1
XML	1
messages	1
Web	1
crawling	1
and/or	1
text	1
processing	1
General	1
archiving,	1
including	1
of	1
relational/tabular	1
data,	1
e.g.	1
for	1
compliance	1
Prominent	1
users	1
On	1
February	1
19,	1
2008,	1
Yahoo!	1
Inc.	1
launched	1
what	1
it	1
claimed	1
was	1
the	1
world's	1
largest	1
Hadoop	1
production	1
application.	1
The	1
Yahoo!	1
Search	1
Webmap	1
is	1
a	1
Hadoop	1
application	1
that	1
runs	1
on	1
a	1
Linux	1
cluster	1
with	1
more	1
than	1
10,000	1
cores	1
and	1
produced	1
data	1
that	1
was	1
used	1
in	1
every	1
Yahoo!	1
web	1
search	1
query.[37]	1
There	1
are	1
multiple	1
Hadoop	1
clusters	1
at	1
Yahoo!	1
and	1
no	1
HDFS	1
file	1
systems	1
or	1
MapReduce	1
jobs	1
are	1
split	1
across	1
multiple	1
datacenters.	1
Every	1
Hadoop	1
cluster	1
node	1
bootstraps	1
the	1
Linux	1
image,	1
including	1
the	1
Hadoop	1
distribution.	1
Work	1
that	1
the	1
clusters	1
perform	1
is	1
known	1
to	1
include	1
the	1
index	1
calculations	1
for	1
the	1
Yahoo!	1
search	1
engine.	1
In	1
June	1
of	1
2009,	1
Yahoo!	1
made	1
the	1
source	1
code	1
of	1
the	1
Hadoop	1
version	1
it	1
runs	1
available	1
to	1
the	1
public	1
via	1
the	1
open-source	1
community.[38]	1
In	1
2010,	1
Facebook	1
claimed	1
that	1
they	1
had	1
the	1
largest	1
Hadoop	1
cluster	1
in	1
the	1
world	1
with	1
21	1
PB	1
of	1
storage.[39]	1
In	1
June	1
of	1
2012,	1
they	1
announced	1
the	1
data	1
had	1
grown	1
to	1
100	1
PB[40]	1
and	1
later	1
that	1
year	1
they	1
announced	1
that	1
the	1
data	1
was	1
growing	1
by	1
roughly	1
half	1
a	1
PB	1
per	1
day.[41]	1
As	1
of	1
2013,	1
Hadoop	1
adoption	1
had	1
become	1
widespread:	1
more	1
than	1
half	1
of	1
the	1
Fortune	1
50	1
used	1
Hadoop.[42]	1
Hadoop	1
hosting	1
in	1
the	1
Cloud	1
Hadoop	1
can	1
be	1
deployed	1
in	1
a	1
traditional	1
onsite	1
datacenter	1
as	1
well	1
as	1
in	1
the	1
cloud.[43]	1
The	1
cloud	1
allows	1
organizations	1
to	1
deploy	1
Hadoop	1
without	1
hardware	1
to	1
acquire	1
or	1
specific	1
setup	1
expertise.[44]	1
Vendors	1
who	1
currently	1
have	1
an	1
offer	1
for	1
the	1
cloud	1
include	1
Microsoft,	1
Amazon,	1
and	1
Google.	1
Hadoop	1
on	1
Microsoft	1
Azure	1
Azure	1
HDInsight[45]	1
is	1
a	1
service	1
that	1
deploys	1
Hadoop	1
on	1
Microsoft	1
Azure.	1
HDInsight	1
uses	1
a	1
Windows-based	1
Hadoop	1
distribution	1
that	1
was	1
jointly	1
developed	1
with	1
Hortonworks	1
and	1
allows	1
programming	1
extensions	1
with	1
.NET	1
(in	1
addition	1
to	1
Java).	1
HDInsight	1
also	1
supports	1
creation	1
of	1
Hadoop	1
clusters	1
using	1
Linux	1
with	1
Ubuntu.[45]	1
By	1
deploying	1
HDInsight	1
in	1
the	1
cloud,	1
organizations	1
can	1
spin	1
up	1
the	1
number	1
of	1
nodes	1
they	1
want	1
and	1
only	1
get	1
charged	1
for	1
the	1
compute	1
and	1
storage	1
that	1
is	1
used.[45]	1
Hortonworks	1
implementations	1
can	1
also	1
move	1
data	1
from	1
the	1
on-premises	1
datacenter	1
to	1
the	1
cloud	1
for	1
backup,	1
development/test,	1
and	1
bursting	1
scenarios.[45]	1
It	1
is	1
also	1
possible	1
to	1
run	1
Cloudera	1
or	1
Hortonworks	1
Hadoop	1
clusters	1
on	1
Azure	1
Virtual	1
Machines.	1
Hadoop	1
on	1
Amazon	1
EC2/S3	1
services	1
It	1
is	1
possible	1
to	1
run	1
Hadoop	1
on	1
Amazon	1
Elastic	1
Compute	1
Cloud	1
(EC2)	1
and	1
Amazon	1
Simple	1
Storage	1
Service	1
(S3).[46]	1
As	1
an	1
example,	1
The	1
New	1
York	1
Times	1
used	1
100	1
Amazon	1
EC2	1
instances	1
and	1
a	1
Hadoop	1
application	1
to	1
process	1
4	1
TB	1
of	1
raw	1
image	1
TIFF	1
data	1
(stored	1
in	1
S3)	1
into	1
11	1
million	1
finished	1
PDFs	1
in	1
the	1
space	1
of	1
24	1
hours	1
at	1
a	1
computation	1
cost	1
of	1
about	1
$240	1
(not	1
including	1
bandwidth).[47]	1
There	1
is	1
support	1
for	1
the	1
S3	1
object	1
store	1
in	1
the	1
Apache	1
Hadoop	1
releases,	1
though	1
this	1
is	1
below	1
what	1
one	1
expects	1
from	1
a	1
traditional	1
POSIX	1
filesystem.	1
Specifically,	1
operations	1
such	1
as	1
rename()	1
and	1
delete()	1
on	1
directories	1
are	1
not	1
atomic,	1
and	1
can	1
take	1
time	1
proportional	1
to	1
the	1
number	1
of	1
entries	1
and	1
the	1
amount	1
of	1
data	1
in	1
them.	1
Amazon	1
Elastic	1
MapReduce	1
Elastic	1
MapReduce	1
(EMR)[48]	1
was	1
introduced	1
by	1
Amazon.com	1
in	1
April	1
2009.	1
Provisioning	1
of	1
the	1
Hadoop	1
cluster,	1
running	1
and	1
terminating	1
jobs,	1
and	1
handling	1
data	1
transfer	1
between	1
EC2(VM)	1
and	1
S3(Object	1
Storage)	1
are	1
automated	1
by	1
Elastic	1
MapReduce.	1
Apache	1
Hive,	1
which	1
is	1
built	1
on	1
top	1
of	1
Hadoop	1
for	1
providing	1
data	1
warehouse	1
services,	1
is	1
also	1
offered	1
in	1
Elastic	1
MapReduce.[49]	1
Support	1
for	1
using	1
Spot	1
Instances[50]	1
was	1
later	1
added	1
in	1
August	1
2011.[51]	1
Elastic	1
MapReduce	1
is	1
fault	1
tolerant	1
for	1
slave	1
failures,[52]	1
and	1
it	1
is	1
recommended	1
to	1
only	1
run	1
the	1
Task	1
Instance	1
Group	1
on	1
spot	1
instances	1
to	1
take	1
advantage	1
of	1
the	1
lower	1
cost	1
while	1
maintaining	1
availability.[53]	1
